{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/MachineLearning_Practice/blob/main/Task_1_Programmers_EN_students_2024_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUuyLVDZ4s8r"
      },
      "source": [
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJ5Q6w2IFHA"
      },
      "source": [
        "**Data URL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64uxt-07IEec"
      },
      "source": [
        "DATA_URL = \"https://www.gutenberg.org/files/1081/1081-h/1081-h.htm\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR33I2GnsRJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ecf1b5c-7285-4c85-82af-b466a28fec55"
      },
      "source": [
        "! pip install -q nltk==3.2.5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m743.8 kB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.1/1.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwrBkeLnHuA1"
      },
      "source": [
        "Loading *NLTK*'s 'wordnet'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTVuMGtood4G",
        "outputId": "2883b7e2-67f2-4865-f835-17f8422c0d94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "lemmatizer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<WordNetLemmatizer>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59q1L9p0H9K9"
      },
      "source": [
        "Downloading the text for the task via `urllib`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uW0fw_h-Pft",
        "outputId": "0e9cbc78-f332-4b1b-f2d2-f9ed1a35e5c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "opener = urllib.request.URLopener({})\n",
        "resource = opener.open(DATA_URL)\n",
        "charset = resource.headers.get_content_charset()\n",
        "print(\"Charset\", charset)\n",
        "raw_text = resource.read()\n",
        "\n",
        "if charset:\n",
        "  raw_text = raw_text.decode(resource.headers.get_content_charset())\n",
        "else:\n",
        "  raw_text = raw_text.decode(\"utf-8\")\n",
        "\n",
        "raw_text[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charset None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!DOCTYPE html>\\r\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\\r\\n<head>\\r\\n    <m'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZiLHQNSITAt"
      },
      "source": [
        "Removing the book ending (Gutenberg legal information)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We4LkyUMPfuq",
        "outputId": "2bdcf67e-135a-400b-f8e1-84be35a4fed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import re\n",
        "\n",
        "clean_pattern = re.compile(\"Subscribe to our email newsletter to hear about new eBooks*\")\n",
        "cleaner_text =  re.sub(clean_pattern, \"\", raw_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \"))\n",
        "cleaner_text[-100:]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ubscribe to our email newsletter to hear about new eBooks.  </div>    </div>      </body>  </html>  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14fYYb5hIpnY"
      },
      "source": [
        "Splitting the text into tokens with a little help from [NLTK](https://nltk.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRNu7jPvN6G_",
        "outputId": "70dbaa86-ab81-49ce-eab7-8804bfd404a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "! unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/;\n",
        "\n",
        "tokens =  word_tokenize(cleaner_text)\n",
        "\"A total of %d 'tokens'\" % len(tokens)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Archive:  /root/nltk_data/corpora/wordnet.zip\n",
            "   creating: /root/nltk_data/corpora/wordnet/\n",
            "  inflating: /root/nltk_data/corpora/wordnet/lexnames  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.verb  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.adv  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/adv.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.verb  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/cntlist.rev  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.adj  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.adj  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/LICENSE  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/citation.bib  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/noun.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/verb.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/README  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.sense  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.noun  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.adv  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.noun  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/adj.exc  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A total of 194209 'tokens'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0qcLFwUqmgt"
      },
      "source": [
        "Now we are about to **lemmatize the tokens**. Please note that for better results we should have first PoS-tagged the text (e.g. with NLTK as well, [please refer to the book and the docs](https://www.nltk.org/book/ch05.html)). `WordNetLemmatizer` would work best with PoS tags provided. However, to make things short and simple, we won't do it as of now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRU4KEBAIyYT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Task \\#1\n",
        "using Python's standard library's `str.isalpha` modify the code below to remove all non-letter tokens from sentences.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U5HH2CDPVUM",
        "outputId": "8379ff6e-67ec-4e58-f54d-d8e16eee76bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "cleaned_tokens = [''.join(filter(str.isalpha, token)) for token in tqdm(tokens) if token.isalpha()]\n",
        "\n",
        "lemmas = [lemmatizer.lemmatize(lemma) for lemma in tqdm(cleaned_tokens)]\n",
        "lemmas[-10:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 194209/194209 [00:00<00:00, 473356.41it/s]\n",
            "100%|██████████| 148272/148272 [00:02<00:00, 59995.24it/s] \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['subscribe',\n",
              " 'to',\n",
              " 'our',\n",
              " 'email',\n",
              " 'newsletter',\n",
              " 'to',\n",
              " 'hear',\n",
              " 'about',\n",
              " 'new',\n",
              " 'eBooks']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHGDhxhNJtTz"
      },
      "source": [
        "Let us check if everything is correct. Total corpus size should be equal to 59087"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5jL4sWyKUnO",
        "outputId": "266deec0-5e62-45bf-8111-dc0a051a5402",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(lemmas)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "148272"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg2e-1hAKiT3"
      },
      "source": [
        "To continue working on this assignment make sure those numbers match!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mci9Nd5hKuJP"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Task #2\n",
        "\n",
        "Using `lemmas`, english NLTK stopwords (`nltk.corpus.stopwords`) and `nltk.FreqDist`, compute **the fraction of the stopwords** in the top-100 most frequent words in the text.\n",
        "\n",
        "E.g. if there were just **3 stopwords in 100** most frequent words in the text, the answer would be **0.03**\n",
        "\n",
        "**Googling how to work with NLTK is encouraged.**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHbtLqkLKfZC",
        "outputId": "4c057cdf-0b33-41e9-e382-e4dac6c06d37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "stopwords.words(\"english\")[:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_freqdist = FreqDist(lemmas)\n",
        "top_50_words = [word for word, freq in lemma_freqdist.most_common(50)]\n",
        "\n",
        "stopwords_count = sum(1 for word in top_50_words if word in STOPWORDS)\n",
        "fraction_stopwords = stopwords_count / 50\n",
        "\n",
        "fraction_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdkB-m63NVLH",
        "outputId": "e9f0d65a-275a-4272-d5da-35f2661bdf37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rRfEl5Kt8-e"
      },
      "source": [
        "Check yourself: stopwords rate in 100 most frequent words in the text should be 0.66"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HChyAdk2Ovx1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Task #3\n",
        "Compute how many tokens occur in the text **strictly greater than** 50 times.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_greater_than_100 = sum(1 for token, freq in lemma_freqdist.items() if freq > 100)\n",
        "\n",
        "tokens_greater_than_100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHoRwUwSNa-y",
        "outputId": "64b9f7a4-208d-48c0-9174-40b179bbff55"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "177"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsagD0a7vSKs"
      },
      "source": [
        "Chek yourself: 149"
      ]
    }
  ]
}