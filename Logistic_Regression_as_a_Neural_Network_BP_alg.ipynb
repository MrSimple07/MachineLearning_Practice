{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/MachineLearning_Practice/blob/main/Logistic_Regression_as_a_Neural_Network_BP_alg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRl66UzjYVlq"
      },
      "source": [
        "# Logistic Regression as a Neural Network\n",
        "(partly following deeplearning.ai lab)\n",
        "\n",
        "    - Initializing parameters\n",
        "    - Calculating the cost function and its gradient\n",
        "    - Using an optimization algorithm (gradient descent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlSzJZcyYVlv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BixMxtmYYVlw"
      },
      "source": [
        "## General Architecture of the algorithm ##\n",
        "\n",
        "\n",
        "Build a Logistic Regression, using a Neural Network mindset.\n",
        "\n",
        "**Logistic Regression is actually a very simple Neural Network!**\n",
        "\n",
        "\n",
        "**Mathematical expression of the algorithm**:\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
        "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$\n",
        "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
        "\n",
        "The cost is then computed by summing over all training examples:\n",
        "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n",
        "\n",
        "Key steps:\n",
        "\n",
        "  -Initialize the parameters of the model\n",
        "\n",
        "  -Learn the parameters for the model by minimizing the cost  \n",
        "\n",
        "  -Use the learned parameters to make predictions (on the test set)\n",
        "  \n",
        "  -Analyse the results and conclude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rFmqFwQYVlw"
      },
      "source": [
        "## Parts of our algorithm ##\n",
        "\n",
        "The main steps for building a Neural Network are:\n",
        "1. Define the model structure (such as number of input features, number of layers, number of neurons in the layer etc.)\n",
        "2. Initialize the model's parameters\n",
        "3. Loop:\n",
        "    - Calculate current loss (forward propagation)\n",
        "    - Calculate current gradient (backward propagation)\n",
        "    - Update parameters (gradient descent)\n",
        "\n",
        "Build 1-3 separately and integrate them into one function we call `model()`.\n",
        "\n",
        "### Sigmoid\n",
        "\n",
        "$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q16NvSTkYVlx"
      },
      "outputs": [],
      "source": [
        "# sigmoid\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "    s = 1./(1.+np.exp(-z))\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9It4n0EvYVlx",
        "outputId": "c395f457-0132-4a3f-ecd3-69d17bddf6d5",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sigmoid([0, 2, 10, -10]) = [0.5        0.88079708 0.11920292 1.        ]\n"
          ]
        }
      ],
      "source": [
        "print (\"sigmoid([0, 2, 10, -10]) = \" + str(sigmoid(np.array([0., 2., -2., 100.]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "KYERIKPHmn0E",
        "outputId": "2266391c-350b-4ff2-d472-2c3baf237bc3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiNklEQVR4nO3deZRU9Z338fe3d6CbHVo2WRRRREEacYkbbqBJYCbBhDwJWQ2ThXkyj5NMzDjHeEzO80ySSTLJ0YmTyWQxOhJjRsM4KKLimERBQEFpFtmXhm526Lbp7lq+zx91wbLTTS9U1a2q/rzOKaruvb9b9eHW7W/f/tW99TN3R0REcl9B2AFERCQ1VNBFRPKECrqISJ5QQRcRyRMq6CIieaIorBcePHiwjxkzplvrvvPOO/Tp0ye1gVIkW7MpV9coV9dla7Z8y7VmzZpD7j6kzYXuHsqtqqrKu2v58uXdXjfdsjWbcnWNcnVdtmbLt1zAam+nrqrLRUQkT6igi4jkCRV0EZE8oYIuIpInVNBFRPJEhwXdzH5uZgfMbH07y83MfmxmW83sTTObmvqYIiLSkc4cof8SmHWG5bcB44PbAuAnZx9LRES6qsMLi9z9ZTMbc4Ymc4CHg/MjV5hZfzMb5u77UxVSRPKXu9McjdMcidMUjdESjRONO7F4nEjMicWdaNyJxk7NdyKxeHB/anmcuDvuEPdT19eA42zaE2Hfyt04TtwB93fb8OftE9MQD75a/NQyAH9P7qTHSUveO7/tFW66qDJVm+89zDvxfehBQX/a3Se1sexp4B/d/Y/B9AvA1919dRttF5A4iqeysrJq0aJF3Qrd0NBAeXl5t9ZNt2zNplxdo1yd4+40RuF4s1N7rJFoYRmNUacxkpifuH/3cXMMWmJOJJ64b4lDJNaq8OUxC+7nTyxh+sDmbr2XM2bMWOPu09paltFL/939p8BPAaZNm+Y33HBDt57npZdeorvrplu2ZlOurlGuBHen5thJdh9uZPeRRnYdSdzvO3aSg/XNHKxvpjkaD1ob0Hx63aICo6KsiL69SuhbVszQvkX0LimiV0khZUUFlBUXUlZ86r7w9HRxYQHFhUZRQQFFBUZRYeK+sMAoCuYXFhjFhcG8YLqwwCgwMAwzgpuxcsWrXH3V1RRYImKBGUZi2en2BYn0BZZYt8ASpdeC5QX27jYxe3ciaTbWTpv2pOO9TEVBrwFGJU2PDOaJSA6JxOJU7zvB+prjbKo9wab99WyqraehOXq6TVGBMXJAL0YM6MXlYwYypKKUoRWlDKkoZd+2Tdx0zXT6lhXTt1cRvYoLO1XY0m1gWQHn9CsLO0ZGpKKgLwYWmtki4ArguPrPRbJfSzTO67uP8tqOI7y24wiv7z5KY0sMgIqyIi46py8fmjqCCedUMHZQH84d1Jth/XpRWNB2kX7p2BYuqKzI5H9BWumwoJvZY8ANwGAz2wt8EygGcPeHgCXA7cBWoBH4TLrCisjZqW+K8MLGAyzbWMfLmw9S3xzFDCZUVnBH1UguHzuQKaP6M6J/r6w4upau6cxZLh/rYLkDX05ZIhFJqVjc+ePWQ/xuzV6WVtfSHI0zpKKU9186jBsvHMoVYwfRr3dx2DElBUL7PnQRSa+G5iiPr9rDL17ZwZ4jJ+nXq5iPTBvFX1w2gstG9aegna4TyV0q6CJ55kRThH97eTu//NNO6pujTBs9gLtnXcTNE4dSWlQYdjxJIxV0kTzRFInxyIpdPLh8K0cbI9x+yTl8/tpxXHbugLCjSYaooIvkgVe3Hebvn3yLHYfe4drxg/m7mRdyych+YceSDFNBF8lhx09G+H9LNrJo1R7OHdibhz87nesuaHu4Scl/KugiOWrtnmN8+dHXqT3RxF9dP46/uekCepWoj7wnU0EXyTHuzsOv7uLb/72BoRVl/O6LVzNlVP+wY0kWUEEXySHRuHPX4+t48o0abrpwKN//yGT69y4JO5ZkCRV0kRzR0Bzlh2uaqD5cw123XMDCGefrXHJ5DxV0kRxwqKGZz/xiFRuPxPne3Eu5Y9qojleSHkcFXSTLHW5o5qP/+io1x07yvy8rVTGXdmmQaJEsVt8U4VO/eI29R0/yy89MZ8pQHYNJ+1TQRbJUUyTG5361mk3763noE1VcOW5Q2JEky+nXvUgWisedv37sDVbtPMI/f3QKMy4cGnYkyQE6QhfJQv/8whaWbajj3g9MZM6UEWHHkRyhgi6SZZ6rruXHL2zhjqqRfPrqMWHHkRyigi6SRbYeaOCux9dx6ch+fOsvJmnUIOkSFXSRLNEUifGFR9ZQWlTAQ5+ooqxY38siXaMPRUWyxHef3czWAw38+nPTGd6/V9hxJAfpCF0kC7y67TA//9MOPnnVaK4dr6+/le5RQRcJWX1ThK/+dh1jB/fh7tsuDDuO5DB1uYiE7NtPb2T/8ZM88cWr6V2iH0npPh2hi4Ro5fbD/Gb1HhZcdx5TNfannCUVdJGQRGNxvrm4mhH9e/GVm8aHHUfygAq6SEgeXbmbTbX1/MP7L9LQcZISKugiITjc0Mz3n9vMNecPZtakc8KOI3lCBV0kBN9bupnGlhj3zZ6oq0ElZVTQRTJsU+0JfrN6D5++egznD60IO47kERV0kQz7/nNvU15SxMIbzw87iuQZFXSRDFq75xjLNtTx+evG0b93SdhxJM+ooItk0Pef28yA3sV89pqxYUeRPNSpgm5ms8xss5ltNbO721h+rpktN7M3zOxNM7s99VFFctvK7Yf5w5ZDfPGG8ygv1RWhknodFnQzKwQeBG4DJgIfM7OJrZr9A/C4u18GzAP+JdVBRXKZu/NPz21maEUpn7xqTNhxJE915gh9OrDV3be7ewuwCJjTqo0DfYPH/YB9qYsokvtWbD/Cqp1HWXjj+fqec0kbc/czNzCbC8xy9zuD6fnAFe6+MKnNMOA5YADQB7jZ3de08VwLgAUAlZWVVYsWLepW6IaGBsrLy7u1brplazbl6ppU5/rB6iZ2nIjx/et7U1LY/fPOs3V7QfZmy7dcM2bMWOPu09pc6O5nvAFzgZ8lTc8HHmjV5i7gb4PHVwEbgIIzPW9VVZV31/Lly7u9brplazbl6ppU5tq4/7iP/vrT/uPn3z7r58rW7eWevdnyLRew2tupq53pcqkBRiVNjwzmJfsc8HjwC+JVoAwY3InnFsl7P315O72KC5l/1eiwo0ie60xBXwWMN7OxZlZC4kPPxa3a7AZuAjCzi0gU9IOpDCqSi/YdO8nitfuYN32UzjuXtOuwoLt7FFgILAU2kjibpdrM7jez2UGzvwU+b2brgMeATwd/Goj0aL/40w4c+JzOO5cM6NTJsO6+BFjSat69SY83AO9LbTSR3HaiKcJ/rNzNBy4dxsgBvcOOIz2ArhQVSZPfrdnLOy0x7rxmXNhRpIdQQRdJA3fnkRW7mDKqP5eM7Bd2HOkhVNBF0uDV7YfZdvAd5l+pM1skc1TQRdLgkRW76N+7mPdfOizsKNKDqKCLpFjdiSaWVtfxkWmjdJm/ZJQKukiKPfbabmJx5+NXnBt2FOlhVNBFUigSi/PYa7u5/oIhjB7UJ+w40sOooIuk0PJNB6g70cwn9GGohEAFXSSFnlizl8HlpcyYMCTsKNIDqaCLpMihhmZe3HSAD00dQVGhfrQk87TXiaTI79fuIxp35laNDDuK9FAq6CIp4O78dvUeJo/sxwWVFWHHkR5KBV0kBar3nWBTbb2OziVUKugiKfDEmr2UFBYwe/KIsKNID6aCLnKWWqJxfr+2hlsurqRf7+Kw40gPpoIucpaWbz7A0caIulskdCroImdp8dp9DOpTwrXnaxhdCZcKushZqG+K8PzGOt5/6TCdey6h0x4ochaWbaijORpn9uThYUcRUUEXORuL1+1jRP9eTD13QNhRRFTQRbrrcEMzf9hyiA9OHk5BgYUdR0QFXaS7lqyvJRZ3dbdI1lBBF+mmxWtrGD+0nIuG6VJ/yQ4q6CLdUHPsJKt2HmX25OGYqbtFsoMKukg3PPPWfgA+qO4WySIq6CLdsLS6lgvPqWDMYA0zJ9lDBV2kiw7WN7N611FuvficsKOIvIcKukgXPb+xDneYeXFl2FFE3kMFXaSLllbXMnJALyYO6xt2FJH3UEEX6YL6pgivbD3MzIvP0dktknU6VdDNbJaZbTazrWZ2dzttPmJmG8ys2sz+I7UxRbLD8s0HaYnFman+c8lCRR01MLNC4EHgFmAvsMrMFrv7hqQ244FvAO9z96NmNjRdgUXCtLS6lkF9Sqgare9ukezTmSP06cBWd9/u7i3AImBOqzafBx5096MA7n4gtTFFwtccjfHSpgPcMrGSQn13i2Qhc/czNzCbC8xy9zuD6fnAFe6+MKnNU8DbwPuAQuA+d3+2jedaACwAqKysrFq0aFG3Qjc0NFBeXt6tddMtW7MpV9e0lWvdwSg/XNPM/6kqZfKQDv+4zViubJGt2fIt14wZM9a4+7Q2F7r7GW/AXOBnSdPzgQdatXkaeBIoBsYCe4D+Z3reqqoq767ly5d3e910y9ZsytU1beX6+hPr/OJ7n/WmSDTzgQLZur3cszdbvuUCVns7dbUzXS41wKik6ZHBvGR7gcXuHnH3HSSO1sd36teNSA6IxZ1lG+q4YcIQSosKw44j0qbOFPRVwHgzG2tmJcA8YHGrNk8BNwCY2WDgAmB76mKKhGvNrqMcfqdFZ7dIVuuwoLt7FFgILAU2Ao+7e7WZ3W9ms4NmS4HDZrYBWA58zd0Ppyu0SKYtra6lpLCAGyYMCTuKSLs69cmOuy8BlrSad2/SYwfuCm4iecXdWVpdy/vOH0RFWXHYcUTapStFRTqwYf8J9h49qe4WyXoq6CIdWFpdR4HBzRP1ZVyS3VTQRTrwXHUt00YPZHB5adhRRM5IBV3kDHYdfodNtfXcqq/KlRyggi5yBkurawHUfy45QQVd5AyWVtcxcVhfRg3sHXYUkQ6poIu040B9E6/vPqqjc8kZKugi7Vi2IRhqbpL6zyU3qKCLtGNpdR2jB/VmQmVF2FFEOkUFXaQNjRHn1W2HNNSc5BQVdJE2rDsYIxJzZup0RckhKugibVhTF2VIRSmXjdJQc5I7VNBFWmmKxHjrUIxbJlZSoKHmJIeooIu08scth2iO6WIiyT0q6CKtLK2upVcRXDVuUNhRRLpEBV0kSTQW5/mNdUweUkhJkX48JLdojxVJsmrnUY42Rqiq7NTYLyJZRQVdJMnS6lpKigq4ZLAGgpbco4IuEnB3lm2o47rxgykr0tktkntU0EUC62tOUHPsJLfq7BbJUSroIoGl1bWJoeYu0tWhkptU0EUCS6truXzMQAb2KQk7iki3qKCLANsPNrDlQIMuJpKcpoIuQuKrcgGNHSo5TQVdhER3y6QRfRk5QEPNSe5SQZcer/Z4E2v3HGPmRHW3SG5TQZceb9mGWgBmTlJBl9ymgi493tLqOsYO7sP4oeVhRxE5Kyro0qMdb4ywYvthbr24UkPNSc5TQZcebdnGOqJxZ5ZOV5Q8oIIuPdqz6/czvF8ZU0b1DzuKyFnrVEE3s1lmttnMtprZ3Wdo92EzczOblrqIIulR3xTh5bcPMWvSMHW3SF7osKCbWSHwIHAbMBH4mJlNbKNdBfAVYGWqQ4qkw4ubDtASi3PbJepukfzQmSP06cBWd9/u7i3AImBOG+2+BXwHaEphPpG0eeatWoZWlFJ17oCwo4ikhLn7mRuYzQVmufudwfR84Ap3X5jUZipwj7t/2MxeAr7q7qvbeK4FwAKAysrKqkWLFnUrdENDA+Xl2XmKWbZmU673ao46f/1iI9eMLOKTE0uzJldHsjUXZG+2fMs1Y8aMNe7edre2u5/xBswFfpY0PR94IGm6AHgJGBNMvwRM6+h5q6qqvLuWL1/e7XXTLVuzKdd7LXlzn4/++tP+p60H21yu7dV12Zot33IBq72dutqZLpcaYFTS9Mhg3ikVwCTgJTPbCVwJLNYHo5LNlqyvZWCfEqaPGRh2FJGU6UxBXwWMN7OxZlYCzAMWn1ro7sfdfbC7j3H3McAKYLa30eUikg2aIjFe3FjHzIsrKSrUmbuSPzrcm909CiwElgIbgcfdvdrM7jez2ekOKJJqf9hyiHdaYsyaNCzsKCIpVdSZRu6+BFjSat697bS94exjiaTPM+v3069XMVefNyjsKCIppb83pUdpicZZtqGOmy+qpFjdLZJntEdLj/LKtkPUN0W5XRcTSR5SQZce5Zm3aikvLeKa8YPDjiKSciro0mM0R2M8W13LzRcNpbSoMOw4Iimngi49xstvH+L4yQhzpowIO4pIWqigS4+xeN0+BvQuVneL5C0VdOkRGluiPL+hjtsvGaazWyRvac+WHmHZhjpORmLMnjw87CgiaaOCLj3C4rX7GNavjMv13S2Sx1TQJe8da2zh5S0H+eDk4RQUaGQiyV8q6JL3nllfSyTm6m6RvKeCLnnvqTdqGDe4DxcP7xt2FJG0UkGXvLb7cCMrdxzhQ1NHaCBoyXsq6JLXfvf6XszgQ1NHhh1FJO1U0CVvxePOE2v2cs35gxnev1fYcUTSTgVd8taKHYepOXaSuVU6OpeeQQVd8tYTq/dSUVrEzIv1VbnSM6igS16qb4qwZP1+PjB5OGXF+mZF6RlU0CUvLXlrP02RuLpbpEdRQZe89PjqvYwb0oep5/YPO4pIxqigS97ZuP8Ea3YdZd7lo3TuufQoKuiSdx5ZsYuSogLuqBoVdhSRjFJBl7xS3xThqTdq+OClwxnQpyTsOCIZpYIueeWpN2p4pyXG/KtGhx1FJONU0CVvuDu/XrGLS0b0Y/LIfmHHEck4FXTJG6/tOMLbdQ3Mv3K0PgyVHkkFXfLGIyt307esiA/qe8+lh1JBl7xQc+wkS97azx3TRtGrRFeGSs+kgi554ed/3AHAZ68ZG3ISkfCooEvOO94Y4bHXdjN78nBG6GtypQfrVEE3s1lmttnMtprZ3W0sv8vMNpjZm2b2gpnpnDHJmEdW7qKxJcaC68aFHUUkVB0WdDMrBB4EbgMmAh8zs4mtmr0BTHP3S4EngO+mOqhIW5oiMX7xp51cf8EQLhqmMUOlZ+vMEfp0YKu7b3f3FmARMCe5gbsvd/fGYHIFoK+4k4x48o0aDjU081c6OhfB3P3MDczmArPc/c5gej5whbsvbKf9A0Ctu3+7jWULgAUAlZWVVYsWLepW6IaGBsrLy7u1brpla7Z8zBWLO3//x5P0KjK+eVVZSs89z8ftlW7Zmi3fcs2YMWONu09rc6G7n/EGzAV+ljQ9H3ignbafIHGEXtrR81ZVVXl3LV++vNvrplu2ZsvHXI+v2u2jv/60P/PWvtQFCuTj9kq3bM2Wb7mA1d5OXS3qxC+EGiD5a+tGBvPew8xuBu4Brnf35s7+thHpjpZonB+9sIVLRvTTEHMigc70oa8CxpvZWDMrAeYBi5MbmNllwL8Cs939QOpjirzXb1bvYe/Rk/ztrRfoMn+RQIcF3d2jwEJgKbAReNzdq83sfjObHTT7HlAO/NbM1prZ4naeTuSsNUViPPDiFi4fM4DrLxgSdhyRrNGZLhfcfQmwpNW8e5Me35ziXCLt+vWru6g70cyP5l2mo3ORJLpSVHLK8cYIP/mfbVw7fjBXjhsUdhyRrKKCLjnlh8+/zbHGFr4+68Kwo4hkHRV0yRkb95/g4Vd38r+uOJdJIzSAhUhrKuiSE9ydby6upl+vYr5664Sw44hkJRV0yQn/9eZ+XttxhK/NvJD+vTX4s0hbVNAl651oivB//3sjk0b05aOXj+p4BZEeqlOnLYqE6f7/2sDBhmYeml9FYYFOUxRpj47QJast21DHE2v28qUbzmPKqP5hxxHJairokrUONzTzjf98k4uH9+WvbxwfdhyRrKcuF8lK7s49T67nxMkoj945hZIiHXuIdEQ/JZKVHn51F89W13LXrRcw4ZyKsOOI5AQVdMk6r+04wree3sDNFw1lwbUaiUiks1TQJavsP36SLz26hnMH9uYHH51Cgc5qEek09aFL1miKxPjiI69zsiXGY5+/kr5lxWFHEskpKuiSFSKxOF9+9HXW7T3GTz5exfhK9ZuLdJW6XCR08bjz1d+u44VNB7h/ziRmTdKQciLdoYIuoXJ37vuvan6/dh9fmzmB+VeODjuSSM5Sl4uEJhZ3flXdwkt7d/FX143jSzecF3YkkZymgi6haIrE+MqiN3hpb5QvzziPr946QcPJiZwlFXTJuGONLSz49Rpe23GEj19YwtdmavQhkVRQQZeMWrvnGF9+9HUO1Dfxo3lT6HdsS9iRRPKGPhSVjHB3fvXKTu546BUAnvjC1cyZMiLkVCL5RUfoknZ7jjRyz1Prefntg9x44VB+8JHJGnVIJA1U0CVtYnHnl6/s5J+WbsYM7vvgRD551Rhdzi+SJiroknLuznMb6vje0s1sPdDAjAlD+PZfXsKI/r3CjiaS11TQJWXiced/3j7Ij1/cwhu7jzFuSB8e+sRUZl58jk5JFMkAFXQ5a40tUZ56Yx///sftbDv4DsP6lfGdD1/Ch6eOpKhQn7uLZIoKunRLPO6s2HGY/3y9hmfe2s87LTEmjejLj+ZN4fZLhlGsQi6ScSro0mnvNEd5ZdthXthYx/MbD3CooZny0iI+cOlw5k4bybTRA9S1IhIiFXRp17HGFlbtPMqqnUdYueMI62uOE4s7FaVFXD9hCLdefA63XFRJr5LCsKOKCCroQqIPfPeRRrYeaGDT/no21Z5g4/56ao6dBKCksIApo/rzhevHcdW4wUwfO1CDNotkoU4VdDObBfwIKAR+5u7/2Gp5KfAwUAUcBj7q7jtTG1W6yt1paI5ysL6ZTUdi1K/bx8H6Zg7UN1N3oondRxrZdbiRQw3Np9cpLDDOG9KHqtED+PiV51J17gAmj+pPWbGOwkWyXYcF3cwKgQeBW4C9wCozW+zuG5KafQ446u7nm9k84DvAR9MROBe5O9G4Ewtu0dP38cR9LFjmfnq6JRanKRKjKRKjOZp43ByJ0xQN7iMxmqIxmiJx6psi1DdFOdEU4cTJKPVNEU40RTlxMkI07u8Gee0NAIoLjaEVZYwa2IsbLxzC6EF9GDWwN+MG92F8ZTmlRSreIrmoM0fo04Gt7r4dwMwWAXOA5II+B7gvePwE8ICZmbs7Kfb4qj388A+N9F7zEg7gcOpF3B0HTr2q47i/O33GNqeXB3NPL393nVPLk6dPvf6pebFYjIIXnsVx4nGIxuPEU74VEgoLjLKiAirKiunbq4iKsmIGl5cwbkgfKsqK6FtWTL9exQztW8q+bZu55drpDCkvpV+vYl2tKZKHrKOaa2ZzgVnufmcwPR+4wt0XJrVZH7TZG0xvC9ocavVcC4AFAJWVlVWLFi3qcuA3DkR5eXcTxUXv/i4yIPnkCjv9DxhGcukyO73oz9axpIm2ps/0eqdeMxKJUFJcDBiFBgUFJO6DW6FZcM977k8vK0h8Y1pRAZQUGiUFUFwIxQVGSSGUFFgwDUVdKMoNDQ2Ul5d3un2mKFfXZGsuyN5s+ZZrxowZa9x9WpsL3f2MN2AuiX7zU9PzgQdatVkPjEya3gYMPtPzVlVVeXctX7682+umW7ZmU66uUa6uy9Zs+ZYLWO3t1NXOnKpQA4xKmh4ZzGuzjZkVAf1IfDgqIiIZ0pmCvgoYb2ZjzawEmAcsbtVmMfCp4PFc4MXgN4mIiGRIhx+KunvUzBYCS0mctvhzd682s/tJHPovBv4d+LWZbQWOkCj6IiKSQZ06D93dlwBLWs27N+lxE3BHaqOJiEhX6HI/EZE8oYIuIpInVNBFRPKECrqISJ7o8ErRtL2w2UFgVzdXHwwc6rBVOLI1m3J1jXJ1XbZmy7dco919SFsLQivoZ8PMVnt7l76GLFuzKVfXKFfXZWu2npRLXS4iInlCBV1EJE/kakH/adgBziBbsylX1yhX12Vrth6TKyf70EVE5M/l6hG6iIi0ooIuIpInsragm9kdZlZtZnEzm9Zq2TfMbKuZbTazme2sP9bMVgbtfhN89W+qM/7GzNYGt51mtraddjvN7K2g3epU52jnNe8zs5qkfLe3025WsB23mtndGcj1PTPbZGZvmtmTZta/nXYZ2WYd/f/NrDR4n7cG+9OYdGVJes1RZrbczDYEPwNfaaPNDWZ2POn9vbet50pTvjO+N5bw42CbvWlmUzOQaULStlhrZifM7G9atcnINjOzn5vZgWAkt1PzBprZMjPbEtwPaGfdTwVttpjZp9pqc0btjXwR9g24CJgAvARMS5o/EVgHlAJjSYyOVNjG+o8D84LHDwFfTHPe7wP3trNsJx2M4JSGPPcBX+2gTWGw/cYBJcF2nZjmXLcCRcHj7wDfCWubdeb/D3wJeCh4PA/4TQbeu2HA1OBxBfB2G7luAJ7O5D7V2fcGuB14hsTIjFcCKzOcrxCoJXEBTsa3GXAdMBVYnzTvu8DdweO729rvgYHA9uB+QPB4QFdeO2uP0N19o7tvbmPRHGCRuze7+w5gK4mBrE8zMwNuJDFgNcCvgL9IV9bg9T4CPJau10iT0wOAu3sLcGoA8LRx9+fcPRpMriAxAlZYOvP/n0Ni/4HE/nRT8H6njbvvd/fXg8f1wEZgRDpfM8XmAA97wgqgv5kNy+Dr3wRsc/fuXol+Vtz9ZRLjQiRL3o/aq0czgWXufsTdjwLLgFldee2sLehnMALYkzS9lz/f2QcBx5IKR1ttUulaoM7dt7Sz3IHnzGxNMFB2piwM/uT9eTt/4nVmW6bTZ0kcybUlE9usM///022C/ek4if0rI4IunsuAlW0svsrM1pnZM2Z2caYy0fF7E/Z+NY/2D67C2maV7r4/eFwLVLbR5qy3W6cGuEgXM3seOKeNRfe4++8znactncz4Mc58dH6Nu9eY2VBgmZltCn6Lpy0b8BPgWyR++L5Fokvos2f7mmeb69Q2M7N7gCjwaDtPk5ZtlkvMrBz4HfA37n6i1eLXSXQpNASfjzwFjM9QtKx9b4LPymYD32hjcZjb7DR3dzNLy/nioRZ0d7+5G6t1ZtDqwyT+zCsKjqraapOSjJYYFPtDQNUZnqMmuD9gZk+S+FP/rH8AOrv9zOzfgKfbWNSZbZnyXGb2aeADwE0edB628Rxp2WatdGUA9L2WwQHQzayYRDF/1N3/s/Xy5ALv7kvM7F/MbLC7p/1LqDrx3qRlv+qk24DX3b2u9YIwtxlQZ2bD3H1/0P10oI02NST6+U8ZSeIzxE7LxS6XxcC84OyDsSR+w76W3CAoEstJDFgNiQGs03XEfzOwyd33trXQzPqYWcWpxyQ+FFzfVttUatVn+ZftvGZnBgBPda5ZwN8Bs929sZ02mdpmWTkAetBH/+/ARnf/QTttzjnVl29m00n8LGfiF01n3pvFwCeDs12uBI4ndTekW7t/LYe1zQLJ+1F79WgpcKuZDQi6SG8N5nVeuj/xPYtPiv+SRB9SM1AHLE1adg+JsxM2A7clzV8CDA8ejyNR6LcCvwVK05Tzl8AXWs0bDixJyrEuuFWT6HbIxPb7NfAW8GawMw1rnS2Yvp3EWRTbMpEteD/2AGuD20Otc2Vym7X1/wfuJ/ELB6As2H+2BvvTuAxso2tIdJW9mbSdbge+cGpfAxYG22YdiQ+Xr87QftXme9MqmwEPBtv0LZLOUktztj4kCnS/pHkZ32YkfqHsByJBDfscic9dXgC2AM8DA4O204CfJa372WBf2wp8pquvrUv/RUTyRC52uYiISBtU0EVE8oQKuohInlBBFxHJEyroIiJ5QgVdRCRPqKCLiOSJ/w9AHGVSxky1qgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = np.arange(-10, 10, 0.1)\n",
        "plt.plot(x, sigmoid(x))\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpd0wqL7YVlx"
      },
      "source": [
        "### Initializing parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC_xYzTEYVly"
      },
      "outputs": [],
      "source": [
        "# initialize_with_zeros\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "\n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "\n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    w = np.zeros((dim,1))\n",
        "    b = 0.\n",
        "\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEQyfQAlYVly",
        "outputId": "29981766-dec1-46d5-8fde-0d237065d39e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w = [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "b = 0.0\n"
          ]
        }
      ],
      "source": [
        "dim = 3\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8qHMDabYVly"
      },
      "source": [
        "### Forward and Backward propagation\n",
        "\n",
        "Implement a function `propagate()` that computes the cost function and its gradient.\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
        "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
        "\n",
        "Here are the two formulas we should use:\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzhXWVi1YVlz"
      },
      "outputs": [],
      "source": [
        "# propagate\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size which equals the number of features\n",
        "    b -- bias, a scalar\n",
        "    X -- data\n",
        "    Y -- true \"label\" vector (containing 0 and 1) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    #print('number of objects = ',len(X))\n",
        "\n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    A = sigmoid(np.dot(w.T,X)+b )                                 # compute activation\n",
        "    cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n",
        "\n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    dw = (1./m)*np.dot(X,(A-Y).T)\n",
        "    db = (1./m)*np.sum(A-Y,axis=1)\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return grads, cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jofXU2o7YVlz",
        "outputId": "9c3486b3-3a52-46b2-9573-8c081c0ce172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dw = [[0.0027004 ]\n",
            " [0.02446984]]\n",
            "db = [0.00151471]\n",
            "cost = [0.00295537]\n"
          ]
        }
      ],
      "source": [
        "w, b, X, Y = np.array([[1.],[-1.]]), 4., np.array([[1.,5.,-1.],[10.,0.,-3.2]]), np.array([[0,1,1]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rjc-2wuYVlz"
      },
      "source": [
        "### Optimization\n",
        "- We have initialized the parameters.\n",
        "- We are able to compute a cost function and its gradient.\n",
        "- Now, we have to update the parameters using gradient descent.\n",
        "\n",
        "For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI5mo-6fYVl0"
      },
      "outputs": [],
      "source": [
        "# optimize\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array\n",
        "    b -- bias, a scalar\n",
        "    X -- data\n",
        "    Y -- true \"label\" vector (containing 0 and 1), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        # Cost and gradient calculation\n",
        "        grads, cost = propagate(w,b,X,Y)\n",
        "\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # update rule\n",
        "        w -=learning_rate*dw\n",
        "        b -=learning_rate*db\n",
        "\n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_Cw0F1mYVl0",
        "outputId": "938c8fa5-076e-4e1c-a64e-fffbea8f1825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.001543\n",
            "Cost after iteration 100: 0.001479\n",
            "Cost after iteration 200: 0.001422\n",
            "Cost after iteration 300: 0.001370\n",
            "Cost after iteration 400: 0.001322\n",
            "Cost after iteration 500: 0.001278\n",
            "Cost after iteration 600: 0.001238\n",
            "Cost after iteration 700: 0.001201\n",
            "Cost after iteration 800: 0.001166\n",
            "Cost after iteration 900: 0.001134\n",
            "Cost after iteration 1000: 0.001104\n",
            "Cost after iteration 1100: 0.001076\n",
            "Cost after iteration 1200: 0.001049\n",
            "Cost after iteration 1300: 0.001024\n",
            "Cost after iteration 1400: 0.001001\n",
            "Cost after iteration 1500: 0.000979\n",
            "Cost after iteration 1600: 0.000958\n",
            "Cost after iteration 1700: 0.000939\n",
            "Cost after iteration 1800: 0.000920\n",
            "Cost after iteration 1900: 0.000902\n",
            "Cost after iteration 2000: 0.000886\n",
            "Cost after iteration 2100: 0.000870\n",
            "Cost after iteration 2200: 0.000854\n",
            "Cost after iteration 2300: 0.000840\n",
            "Cost after iteration 2400: 0.000826\n",
            "Cost after iteration 2500: 0.000813\n",
            "Cost after iteration 2600: 0.000800\n",
            "Cost after iteration 2700: 0.000787\n",
            "Cost after iteration 2800: 0.000776\n",
            "Cost after iteration 2900: 0.000764\n",
            "Cost after iteration 3000: 0.000754\n",
            "Cost after iteration 3100: 0.000743\n",
            "Cost after iteration 3200: 0.000733\n",
            "Cost after iteration 3300: 0.000723\n",
            "Cost after iteration 3400: 0.000714\n",
            "Cost after iteration 3500: 0.000705\n",
            "Cost after iteration 3600: 0.000696\n",
            "Cost after iteration 3700: 0.000688\n",
            "Cost after iteration 3800: 0.000680\n",
            "Cost after iteration 3900: 0.000672\n",
            "Cost after iteration 4000: 0.000664\n",
            "Cost after iteration 4100: 0.000657\n",
            "Cost after iteration 4200: 0.000650\n",
            "Cost after iteration 4300: 0.000643\n",
            "Cost after iteration 4400: 0.000636\n",
            "Cost after iteration 4500: 0.000629\n",
            "Cost after iteration 4600: 0.000623\n",
            "Cost after iteration 4700: 0.000617\n",
            "Cost after iteration 4800: 0.000611\n",
            "Cost after iteration 4900: 0.000605\n",
            "w = [[ 0.97557009]\n",
            " [-1.22487345]]\n",
            "b = [3.99972589]\n",
            "dw = [[0.00031994]\n",
            " [0.00333921]]\n",
            "db = [-0.00013662]\n"
          ]
        }
      ],
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 5000, learning_rate = 0.005, print_cost = True)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WveKLLR6YVl1"
      },
      "source": [
        "Implement the `predict()` function. There are two steps to compute predictions:\n",
        "\n",
        "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
        "\n",
        "2. Convert the entries of A into 0 (if activation <= 0.5) or 1 (if activation > 0.5), store the predictions in a vector `Y_prediction`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6p8pd8pYVl1"
      },
      "outputs": [],
      "source": [
        "# predict\n",
        "\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array\n",
        "    b -- bias, a scalar\n",
        "    X -- data\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    # Compute vector \"A\" predicting the probabilities\n",
        "    A = sigmoid(np.dot(w.T,X)+b)\n",
        "\n",
        "    for i in range(A.shape[1]):\n",
        "\n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        if (A[0,i]<=0.5):\n",
        "            Y_prediction[0][i]=0\n",
        "        else:\n",
        "            Y_prediction[0][i]=1\n",
        "\n",
        "    return Y_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeFbe51nYVl2",
        "outputId": "11efb7dc-cb8e-4fd8-e89f-fc269409809c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predictions = [[1. 1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "w = np.array([[0.1124579],[0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
        "print (\"predictions = \" + str(predict(w, b, X)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "IzJOsYM2YVl2",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "\n",
        "Already have:\n",
        "- Initialize (w,b)\n",
        "- Optimize the loss iteratively to learn parameters (w,b):\n",
        "    - computing the cost and its gradient\n",
        "    - updating the parameters using gradient descent\n",
        "- Use the learned (w,b) to predict the labels for a given set of examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIHsdqPgYVl2"
      },
      "source": [
        "## Merge it all into a model ##\n",
        "\n",
        "    - Y_prediction_test for the predictions on the test set\n",
        "    - Y_prediction_train for the predictions on the train set\n",
        "    - w, costs, grads for the outputs of optimize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMVbePtZYVl2"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function we've implemented previously\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize parameters with zeros\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMOjXzwNYVl3"
      },
      "source": [
        "Run the following cell to train your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NWUYiqoYVl3"
      },
      "outputs": [],
      "source": [
        "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.001, print_cost = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v42GrLJIYVl3"
      },
      "source": [
        "Bibliography:\n",
        "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz55zNblYVl4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}