{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMclqfeaa+LFTg+drgz1Ns2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/MachineLearning_Practice/blob/main/RandomNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqpFrhMbaS4a",
        "outputId": "dc9dee42-06e9-4f88-fcbd-12d5a5758407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#Topic identification\n",
        "\n",
        "import nltk\n",
        "import requests\n",
        "from nltk.corpus import stopwords\n",
        "from gensim import corpora, models\n",
        "from pprint import pprint\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With LDA"
      ],
      "metadata": {
        "id": "DWoeF6Aefgj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Random text\n",
        "document = 'It is a very broad definition that can be summed in the following way. The word technology is derived from ‘techne’ and logia, two Greek words.Techne means science. It also stands for the philosophical meaning of using scientific knowledge. Logia means the art of using skills and different  techniques to accomplish a particular motive. After combining both the Greek words, the perfect definition is derived as — The art of using specific  scientific skills and knowledge to accomplish a motive of making an approach better and easier.'\n",
        "\n",
        "stop_words= set(stopwords.words('english'))\n",
        "tokens =[word for word in nltk.word_tokenize(document.lower()) if word.isalpha() and word not in stop_words]\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary([tokens])\n",
        "corpus = [dictionary.doc2bow(tokens)]\n",
        "\n",
        "\n",
        "#LDA model - Latent Dirichlet Allocation\n",
        "lda_model = models.LdaModel(corpus, num_topics =1, id2word = dictionary, passes =10)\n",
        "pprint(lda_model.print_topics())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrqPmsnRaoS-",
        "outputId": "288f17b5-dcce-4a93-8046-b1de0a71eda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.047*\"using\" + 0.035*\"knowledge\" + 0.035*\"means\" + 0.035*\"skills\" + '\n",
            "  '0.035*\"scientific\" + 0.035*\"motive\" + 0.035*\"logia\" + 0.035*\"accomplish\" + '\n",
            "  '0.035*\"definition\" + 0.035*\"greek\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP with spaCy\n",
        "\n",
        "- Using for basic text processing and pattern matching\n",
        "- Building machine learning models with text\n",
        "- Representing text with word embeddings"
      ],
      "metadata": {
        "id": "AlxcJQ00jGHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp('The quick brown foxes are jumping over the lazy dogs. They have been running through the fields and forests, exploring the beauty of nature.')"
      ],
      "metadata": {
        "id": "ZWVlo6dAdj6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F3AZg4Vjwk6",
        "outputId": "559980c4-0a29-4abb-c319-89893b7751bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "quick\n",
            "brown\n",
            "foxes\n",
            "are\n",
            "jumping\n",
            "over\n",
            "the\n",
            "lazy\n",
            "dogs\n",
            ".\n",
            "They\n",
            "have\n",
            "been\n",
            "running\n",
            "through\n",
            "the\n",
            "fields\n",
            "and\n",
            "forests\n",
            ",\n",
            "exploring\n",
            "the\n",
            "beauty\n",
            "of\n",
            "nature\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatizing and stop words\n",
        "\n",
        "for token in doc:\n",
        "  print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfyZ-g3PjzuL",
        "outputId": "2beb231e-0662-4d35-fde9-77916f595517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\t\tthe\t\tTrue\n",
            "quick\t\tquick\t\tFalse\n",
            "brown\t\tbrown\t\tFalse\n",
            "foxes\t\tfox\t\tFalse\n",
            "are\t\tbe\t\tTrue\n",
            "jumping\t\tjump\t\tFalse\n",
            "over\t\tover\t\tTrue\n",
            "the\t\tthe\t\tTrue\n",
            "lazy\t\tlazy\t\tFalse\n",
            "dogs\t\tdog\t\tFalse\n",
            ".\t\t.\t\tFalse\n",
            "They\t\tthey\t\tTrue\n",
            "have\t\thave\t\tTrue\n",
            "been\t\tbe\t\tTrue\n",
            "running\t\trun\t\tFalse\n",
            "through\t\tthrough\t\tTrue\n",
            "the\t\tthe\t\tTrue\n",
            "fields\t\tfield\t\tFalse\n",
            "and\t\tand\t\tTrue\n",
            "forests\t\tforest\t\tFalse\n",
            ",\t\t,\t\tFalse\n",
            "exploring\t\texplore\t\tFalse\n",
            "the\t\tthe\t\tTrue\n",
            "beauty\t\tbeauty\t\tFalse\n",
            "of\t\tof\t\tTrue\n",
            "nature\t\tnature\t\tFalse\n",
            ".\t\t.\t\tFalse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pattern Matching"
      ],
      "metadata": {
        "id": "c1L-UXmDmK76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab, attr ='LOWER')"
      ],
      "metadata": {
        "id": "i9zpawJ4lEBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
        "patterns = [nlp(text) for text in terms]\n",
        "matcher.add(\"TerminologyList\", patterns)"
      ],
      "metadata": {
        "id": "brBgGmcnmDfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
        "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
        "               \"Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.\")\n",
        "matches = matcher(text_doc)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0OZnxcWmOgM",
        "outputId": "2f5e6a82-3aa6-4963-f617-039d52a9daa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_id, start, end = matches[0]\n",
        "print(nlp.vocab.strings[match_id], text_doc[start:end])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yuW_iWfmUmw",
        "outputId": "d9bfc4d1-8716-4d11-d497-e491336ad320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TerminologyList iPhone 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part of Speech (POS) tagging\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYtaUxQHmY9b",
        "outputId": "b66e99df-1355-46f0-8658-cca04f5433d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DET\n",
            "quick ADJ\n",
            "brown ADJ\n",
            "foxes NOUN\n",
            "are AUX\n",
            "jumping VERB\n",
            "over ADP\n",
            "the DET\n",
            "lazy ADJ\n",
            "dogs NOUN\n",
            ". PUNCT\n",
            "They PRON\n",
            "have AUX\n",
            "been AUX\n",
            "running VERB\n",
            "through ADP\n",
            "the DET\n",
            "fields NOUN\n",
            "and CCONJ\n",
            "forests NOUN\n",
            ", PUNCT\n",
            "exploring VERB\n",
            "the DET\n",
            "beauty NOUN\n",
            "of ADP\n",
            "nature NOUN\n",
            ". PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NER\n",
        "for ent in text_doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzfRY0UEoHA0",
        "outputId": "e046be9f-dfd8-414d-88d9-3208f54fc5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Plus and last year DATE\n",
            "iPhone XS ORG\n",
            "Google Pixel ORG\n",
            "3 CARDINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Embeddings\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, token.vector[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaXjyJlWrimu",
        "outputId": "f723234f-de19-47fe-e324-f695008208e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The [ 0.85183746 -0.29544368  0.6358199   1.5570921  -0.6759108 ]\n",
            "quick [ 0.8100312 -1.2707639  1.0589166  1.1848149  0.2042971]\n",
            "brown [ 0.59353954 -1.1236539   0.06441484  0.52956516  0.09440499]\n",
            "foxes [-0.48174232  0.93088746 -0.5199428   0.48028803  1.6327056 ]\n",
            "are [-0.9376364  -0.8994892  -0.07146559 -0.15708345 -0.4286011 ]\n",
            "jumping [-0.04427019  0.78260726  0.01712537  0.27085647 -1.3030365 ]\n",
            "over [ 0.08426314  0.90991944  0.04415579 -1.2731304  -0.38922563]\n",
            "the [ 1.0078084   1.1961887   0.66080004  1.0293958  -0.02119049]\n",
            "lazy [-0.3079457  -1.1043328   0.40270567  0.05805285 -0.73500675]\n",
            "dogs [-1.020669    1.4565394   0.8399615   0.56590575  0.24345604]\n",
            ". [-0.0751619  -0.62901324 -0.07042383 -1.621114   -0.543389  ]\n",
            "They [-1.6231596  -0.48582357 -0.6938442   0.03227412 -0.39192843]\n",
            "have [-0.32260102 -0.44645384  0.35678056 -0.99264956  0.360287  ]\n",
            "been [-0.21974844 -0.17340861  0.6379559   0.29546565 -1.5078602 ]\n",
            "running [-0.47258344  0.65876794  0.8013135   0.5232028  -0.6259217 ]\n",
            "through [ 0.9440706   1.2979771  -0.30451497 -0.77263975  0.17601797]\n",
            "the [ 1.9075193   0.05665952 -0.10438752  0.4890826  -0.35978764]\n",
            "fields [-1.155302    1.4976478  -0.27144367 -0.18735719  1.2726033 ]\n",
            "and [ 0.4875502  -0.6452007  -1.4982074   0.15568876 -0.09102746]\n",
            "forests [-0.3605336   1.9380891  -0.6962267   0.76329327  2.3596478 ]\n",
            ", [-1.2110195   0.16070631 -0.01924065 -0.87946546 -0.0574486 ]\n",
            "exploring [-1.4187955   0.6713905  -0.5447438   0.30314416  0.520124  ]\n",
            "the [1.4178138  0.41218245 0.22626558 1.0599986  0.08414841]\n",
            "beauty [-0.8724121   0.2925285   0.06854364 -0.8631899   0.5352901 ]\n",
            "of [ 1.1542355  -0.02260794 -1.2703452  -0.66299236 -0.7661575 ]\n",
            "nature [-1.6761302   0.71091956 -0.29541817 -0.7517429   0.6148532 ]\n",
            ". [-0.70964754 -0.38040954  0.2084322  -1.4010756  -0.39970112]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK"
      ],
      "metadata": {
        "id": "DLKA9w4nk0tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "dg1Kdqlwukm8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DaQeKXbm6RF",
        "outputId": "04d12547-2f56-4cd5-bb08-0c1a9358885f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW and using it for Classification of the text"
      ],
      "metadata": {
        "id": "0dWa2JN1nKfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"I love this product\", \"This is a bad product\", \"I dislike this\", \"This is the best!\"]\n",
        "labels = [1, 0, 0, 1]"
      ],
      "metadata": {
        "id": "3mBWAdRjnAFV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [' '.join(word_tokenize(text)) for text in texts]\n",
        "\n",
        "#BoW vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit_transform(tokens)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(bow, labels, test_size = 0.3, random_state =23)\n",
        "print(bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_zZDoAinIWF",
        "outputId": "cbaa2499-8976-4bc0-8548-fa3b54f46555"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 4)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 5)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 0)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 2)\t1\n",
            "  (3, 7)\t1\n",
            "  (3, 3)\t1\n",
            "  (3, 6)\t1\n",
            "  (3, 1)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = classifier.predict(X_test)\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5niAgBnBoGI-",
        "outputId": "e6470aab-bf26-4335-d557-506d159805c5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW for sentiment analysis"
      ],
      "metadata": {
        "id": "HVmq6f0Fpxub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import movie_reviews\n",
        "import random"
      ],
      "metadata": {
        "id": "H8oJUNZQpOoe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XxcZSsaq8Kb",
        "outputId": "dff93c67-362e-48c2-9946-b0588dd7b99e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)"
      ],
      "metadata": {
        "id": "ng7pLcLtq_w0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [''.join(doc) for doc, _ in documents]\n",
        "labels = [1 if category =='pos' else 0 for _, category in documents]"
      ],
      "metadata": {
        "id": "EAfRvzkprXLA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BoW\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit_transform(texts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(bow, labels, test_size =0.3, random_state=23)\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = classifier.predict(X_test)\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UxxtbkCuR-z",
        "outputId": "bf2fcdf0-d081-427a-9cb4-978ccae40da3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tY6voLRKu3sR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}